#!/bin/bash

set -ex

function run_dml {
    local project=$1
    local dataset=$2
    local query_location=$3

    dml_tmp=$(mktemp)
    cat "$query_location" > "$dml_tmp"
    bq query \
        --use_legacy_sql=false \
        --project_id="$project" \
        --dataset_id="$dataset" \
        < "$dml_tmp"
}

function truncate_hotlist_backfill {
    local project=$1
    local dataset=$2
    truncate_tmp=$(mktemp)
    echo "TRUNCATE TABLE ${project}.${dataset}.glam_hotlist_backfilled" >> $truncate_tmp
    bq query \
        --max_rows=0 \
        --use_legacy_sql=false \
        $additional_arguments \
        --project_id="$project" \
        --dataset_id="$dataset" \
        < "$truncate_tmp"
}

function run_query {
    local project=$1
    local dataset=$2
    local dest_table=$3
    local query_location=$4
    local additional_arguments="${6:---append_table}"

    # add an option to write to a time-partitioned table
    if $time_partition; then
        destination_table="${destination_table}\$${SUBMISSION_DATE//-/}"
    fi
    echo "running $query_location"
    local tmp
    tmp=$(mktemp)
    cat "$query_location" > "$tmp"
    bq query \
        --max_rows=0 \
        --use_legacy_sql=false \
        $additional_arguments \
        --project_id="$project" \
        --dataset_id="$dataset" \
        --destination_table="$dest_table" \
        < "$tmp"
}

echo "Updating probe hotlist..."

src_project=${SRC_PROJECT:-moz-fx-data-shared-prod}
src_dataset=${PROD_DATASET:-dev_telemetry_derived}
dst_dataset=${DATASET:-dev_telemetry_derived}
sql_dir=${SQL_DIR:-sql}
sql_dir=${sql_dir%/}

gcloud config set project $src_project
cd "$(dirname "$0")/../.."
# ensure the sql directory exists
query_dir=$sql_dir/$src_project/$dst_dataset/glam_hotlist_backfilled
mkdir -p $query_dir

# Init tables
hotlist_backfilled_init_query_loc=$sql_dir/$src_project/$dst_dataset/glam_hotlist_backfilled/init.sql
hotlist_init_query_loc=$sql_dir/$src_project/$dst_dataset/glam_hotlist/init.sql
run_dml $src_project $dst_dataset $hotlist_backfilled_init_query_loc
run_dml $src_project $dst_dataset $hotlist_init_query_loc

# Generate queries
queries=$(python3 bigquery_etl/glam/generate_hotlist_backfill.py \
        --probe_name=${1} \
        --process='all' \
        --query_dest_folder=${query_dir})

# Execute queries, load data to glam_hotlist_backfilled table
truncate_hotlist_backfill $src_project $dst_dataset

# Cast queries into an array of query locations
queries=($queries)
# The first n-1 elements of the array are backfill queries
backfill_queries=${queries[@]::${#queries[@]}-1}
# The last one updates the hotlist
update_hotlist_query=${queries[${#queries[@]}-1]}
for i in $backfill_queries
do
    run_query $src_project $dst_dataset "glam_hotlist_backfilled" $i
done

# We can now update the hotlist after new probes have been backfilled
run_dml $src_project $dst_dataset $update_hotlist_query