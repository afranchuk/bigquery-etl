"""hotlist probe backfill query generator."""
import argparse
import gzip
import json
import os
import urllib.request
from datetime import date
from urllib.request import Request, urlopen

from dateutil.relativedelta import relativedelta
from google.cloud import bigquery
from jinja2 import Environment, PackageLoader

from bigquery_etl.format_sql.formatter import reformat
from bigquery_etl.util.glam_probe_utils import query_hotlist

PROBE_INFO_SERVICE = (
    "https://probeinfo.telemetry.mozilla.org/firefox/all/main/all_probes"
)


def _build_probe_location(probe_name, metric_type, process, is_keyed):
    keyed_path = "keyed_" if is_keyed else ""
    probe_location = (
        f"payload.{keyed_path}{metric_type}s.{probe_name}"
        if process == "parent" and metric_type == "histogram"
        else f"payload.processes.{ process }.{metric_type}s.{ probe_name}"
    )
    return probe_location


def get_most_used_probes():
    """Fetch and provide probes that have been used lately in Glam.

    Important: The API does not distinguish from different products (e.g.: fenix, desktop),
    so make sure the list of probes from this function goes through a filter
    to avoid computing probes for a different product.
    """
    glam_usage_service = "https://glam.telemetry.mozilla.org/api/v1/usage/"
    from_date = (date.today() - relativedelta(months=3)).strftime("%Y%m%d")
    url_req = Request(
        f"{glam_usage_service}?fromDate={from_date}& \
            fields=probe_name&actionType=PROBE_SEARCH&agg=count",
        None,
        {},
    )
    probe_names = []
    with urlopen(url_req) as url:
        data_str = gzip.decompress(url.read()).decode()
        data = json.loads(data_str)
        probe_names = [entry["probe_name"] for entry in data]

    return set(probe_names)


def _get_all_processes_for_probe(project, probe_name, metric_type, is_keyed):
    client = bigquery.Client(project)
    table = client.get_table("telemetry_stable.main_v4")
    main_summary_schema = [field.to_api_repr() for field in table.schema]

    processes = []
    for item in main_summary_schema:
        if item["name"] == "payload":
            for field in item["fields"]:
                if metric_type == "histogram":
                    if (not is_keyed and field["name"] == "histograms") or (
                        is_keyed and field["name"] == "keyed_histograms"
                    ):
                        for subfield in field["fields"]:
                            if subfield["name"] == probe_name:
                                processes.append("parent")
                                break
                if field["name"] == "processes":
                    for process in field["fields"]:
                        if process["name"] in ["parent", "content", "gpu"]:
                            for probe_type_field in process["fields"]:
                                for subfield in probe_type_field["fields"]:
                                    if subfield["name"] == probe_name:
                                        processes.append(process["name"])
                                        break
    return processes


def _get_probe_details(probe_name):
    with urllib.request.urlopen(PROBE_INFO_SERVICE) as url:
        data = json.loads(url.read())

    for key in data.keys():
        metric_type = data[key]["type"]
        probe = key.replace(f"{metric_type}/", "").replace(".", "_").lower()
        if probe == probe_name:
            channel = "nightly"
            if "nightly" not in data[key]["history"]:
                channel = "beta"

                if "beta" not in data[key]["history"]:
                    channel = "release"
            data_details = data[key]["history"][channel][0]["details"]
            metric_kind = f'{data_details["kind"]}'
            is_keyed = data_details["keyed"]
            bucket_details = None
            if metric_type == "histogram":
                bucket_details = {
                    "n_buckets": int(eval(str(data_details["n_buckets"]))),
                    "min": int(eval(str(data_details["low"]))),
                    "max": int(eval(str(data_details["high"]))),
                }
            return (metric_type, metric_kind, bucket_details, is_keyed)
    raise Exception(f"Could not find probe {probe} from {PROBE_INFO_SERVICE}")


def _gen_query_for_probe(
    probe_name, backfill_days, source_table, force_process, project, query_dest_folder
) -> list[str]:
    """Generate SQL queries for probe and return their locations."""
    header = (
        "-- Query generated by: "
        "python3 -m bigquery_etl.glam.hotlist_probe_backfill "
        f"--source-table {source_table} "
        f"--probe_name {probe_name} "
        f"--days {backfill_days} "
        f"--process {force_process} "
        f"--project {project}"
    )

    (metric_type, metric_kind, bucket_details, is_keyed) = _get_probe_details(
        probe_name=probe_name
    )
    if metric_type not in ["histogram", "scalar"]:
        raise ValueError(f"Invalid metric type: {metric_type}")

    processes = (
        [force_process]
        if force_process != "all"
        else _get_all_processes_for_probe(project, probe_name, metric_type, is_keyed)
    )
    generated_queries = []
    template_env = Environment(
        loader=PackageLoader("bigquery_etl", "glam/templates/hotlist_backfill")
    )
    for process in processes:
        probe_location = _build_probe_location(
            probe_name, metric_type, process, is_keyed
        )

        if metric_type == "scalar":
            query_template = template_env.get_template("glam_backfill_scalars.sql")
            query = reformat(
                query_template.render(
                    header=header,
                    project=project,
                    source_table=source_table,
                    days=backfill_days,
                    metric=probe_name,
                    metric_type=metric_type,
                    process=process,
                    is_keyed=is_keyed,
                    probe_location=probe_location,
                )
            )

        elif metric_type == "histogram":
            query_template = template_env.get_template("glam_backfill_histograms.sql")
            query = reformat(
                query_template.render(
                    header=header,
                    project=project,
                    source_table=source_table,
                    days=backfill_days,
                    metric=probe_name,
                    metric_kind=metric_kind,
                    process=process,
                    is_keyed=is_keyed,
                    probe_location=probe_location,
                    first_bucket=bucket_details["min"],
                    last_bucket=bucket_details["max"],
                    num_buckets=bucket_details["n_buckets"],
                )
            )
        if query_dest_folder:
            out_folder = os.path.join(
                query_dest_folder, metric_type, probe_name, process
            )
            os.makedirs(out_folder, exist_ok=True)
            out_file = os.path.join(out_folder, "query.sql")
            with open(out_file, "w") as f:
                f.write(query)
            # print(f"Saved query {out_file}")
            generated_queries.append(out_file)

    return generated_queries


def _gen_update_hotlist_query(dataset, metrics, query_dest_folder):
    template_location = "glam/templates/hotlist_update"
    template_name = "glam_update_hotlist.sql"
    template_env = Environment(loader=PackageLoader("bigquery_etl", template_location))
    query_template = template_env.get_template(template_name)
    query = reformat(query_template.render(dataset=dataset, metrics=list(metrics)))
    if query_dest_folder:
        out_folder = os.path.join(query_dest_folder, "new_hotlist")
        os.makedirs(out_folder, exist_ok=True)
        out_file = os.path.join(out_folder, "query.sql")
        with open(out_file, "w") as f:
            f.write(query)
        return out_file


def _find_probes_to_backfill(new_hotlist, current_hotlist) -> list[tuple[str, int]]:
    """Return all probes that just made to the hotlist and need backfilling."""
    probes_to_backfill = new_hotlist - current_hotlist
    return [(probe_name, 180) for probe_name in probes_to_backfill]


def main():
    """Generate queries for backfilling recent hot probes and updating hotlist."""
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--probe_name",
        type=str,
        help="Single probe to backfill. If empty \
        this script will find a list of new hot probes to backfill.",
    )
    parser.add_argument(
        "--days",
        type=int,
        help="Number of days to backfill. If omitted \
            this script will infer this for each probe to be backfilled.",
    )
    parser.add_argument(
        "--dst_dataset",
        type=str,
        default="dev_telemetry_derived",
        help="The destination dataset.",
    )
    parser.add_argument(
        "--process",
        type=str,
        default="all",
        choices=["parent", "content", "gpu", "all"],
        help="Choice of process [parent, content, gpu, all].",
    )
    parser.add_argument(
        "--source_table",
        type=str,
        help="Name of the Source table.",
        default="telemetry_derived.main_1pct_v1",
    )
    parser.add_argument(
        "--project",
        type=str,
        default="moz-fx-data-shared-prod",
    )
    parser.add_argument(
        "--query_dest_folder",
        type=str,
        required=True,
        help="Folder to store the backfill queries (1 per probe).",
    )
    args = parser.parse_args()

    # Extract hotlists to work with
    new_hotlist = get_most_used_probes()
    current_hotlist = query_hotlist()

    # Probes to be backfilled
    probe_names = (
        [args.probe_name]
        if args.probe_name
        else _find_probes_to_backfill(new_hotlist, current_hotlist)
    )
    generated_queries = []
    for (probe_name, days) in probe_names:
        backfill_days = args.days if args.days else days
        queries_loc = _gen_query_for_probe(
            probe_name,
            backfill_days,
            args.source_table,
            args.process,
            args.project,
            args.query_dest_folder,
        )
        generated_queries += queries_loc

    # Generate query to update new hotlist on BQ
    new_hotlist_query_loc = _gen_update_hotlist_query(
        args.dst_dataset, new_hotlist, args.query_dest_folder
    )
    generated_queries.append(new_hotlist_query_loc)
    for query in generated_queries:
        print(query)


if __name__ == "__main__":
    main()
